{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140b33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2025add7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5e1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "query, key, value = torch.randn(2,3,8, device = device), torch.randn(2,3,8, device = device), torch.randn(2,3,8, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b78c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3102,  0.3511, -0.0225,  0.0732,  1.0476,  0.0821, -0.1841,\n",
       "          -0.7381],\n",
       "         [-1.4471,  0.3482,  0.0497,  0.1377,  1.0303,  0.0467, -0.3995,\n",
       "          -0.7993],\n",
       "         [-0.9609,  0.0229, -0.9022,  0.2971,  0.9908, -0.3394,  0.3428,\n",
       "          -0.2445]],\n",
       "\n",
       "        [[ 0.1529,  0.4307,  0.3648,  0.7064, -0.2333, -0.0723, -0.3468,\n",
       "           0.0796],\n",
       "         [ 0.3643,  0.3583, -0.3079,  0.4511, -0.1409, -0.3006, -0.7080,\n",
       "           0.5138],\n",
       "         [ 0.2387,  0.3994,  0.2214,  0.8081, -0.2716, -0.2258, -0.4523,\n",
       "           0.0724]]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.scaled_dot_product_attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d66172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "def benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt = \"f(*args, **kwargs)\", globals = {\"args\" : args, \"kwargs\" : kwargs, 'f' : f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb6afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embed_dimension = 32\n",
    "max_sequence_len = 1024\n",
    "num_heads = 32\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5a823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "key = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)\n",
    "value = torch.rand(batch_size, num_heads, max_sequence_len, embed_dimension, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39204bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default implementation runs in 274997.736 microseconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b766b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 1.96 GiB total capacity; 328.13 MiB already allocated; 1.58 GiB free; 342.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m backend_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     SDPBackend\u001b[38;5;241m.\u001b[39mMATH: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_math\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_flash\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_mem_efficient\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m      7\u001b[0m     SDPBackend\u001b[38;5;241m.\u001b[39mFLASH_ATTENTION: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_math\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_flash\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_mem_efficient\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m      8\u001b[0m     SDPBackend\u001b[38;5;241m.\u001b[39mEFFICIENT_ATTENTION: {\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_math\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_flash\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_mem_efficient\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sdp_kernel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbackend_map[SDPBackend\u001b[38;5;241m.\u001b[39mMATH]):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe math implementation runs in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark_torch_function_in_microseconds(F\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention,\u001b[38;5;250m \u001b[39mquery,\u001b[38;5;250m \u001b[39mkey,\u001b[38;5;250m \u001b[39mvalue)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m microseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sdp_kernel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbackend_map[SDPBackend\u001b[38;5;241m.\u001b[39mFLASH_ATTENTION]):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mbenchmark_torch_function_in_microseconds\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark_torch_function_in_microseconds\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mTimer(\n\u001b[1;32m      4\u001b[0m         stmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf(*args, **kwargs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mglobals\u001b[39m \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m : args, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m : kwargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m : f}\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t0\u001b[38;5;241m.\u001b[39mblocked_autorange()\u001b[38;5;241m.\u001b[39mmean \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e6\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Pytorch/lib/python3.11/site-packages/torch/utils/benchmark/utils/timer.py:394\u001b[0m, in \u001b[0;36mTimer.blocked_autorange\u001b[0;34m(self, callback, min_run_time)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mblocked_autorange\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     callback: Optional[Callable[[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], NoReturn]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    356\u001b[0m     min_run_time: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m    357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m common\u001b[38;5;241m.\u001b[39mMeasurement:\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Measure many replicates while keeping timer overhead to a minimum.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    At a high level, blocked_autorange executes the following pseudo-code::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m        (mean, median, etc.)\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_block_size(min_run_time)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime_hook\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeit(number)\n",
      "File \u001b[0;32m~/miniconda3/envs/Pytorch/lib/python3.11/site-packages/torch/utils/benchmark/utils/timer.py:311\u001b[0m, in \u001b[0;36mTimer._estimate_block_size\u001b[0;34m(self, min_run_time)\u001b[0m\n\u001b[1;32m    309\u001b[0m number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     time_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeit(number)\n\u001b[1;32m    312\u001b[0m     relative_overhead \u001b[38;5;241m=\u001b[39m overhead \u001b[38;5;241m/\u001b[39m time_taken\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m relative_overhead \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m time_taken \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_run_time \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Pytorch/lib/python3.11/site-packages/torch/utils/benchmark/utils/timer.py:256\u001b[0m, in \u001b[0;36mTimer._timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_timeit\u001b[39m(\u001b[38;5;28mself\u001b[39m, number: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# Even calling a timer in C++ takes ~50 ns, so no real operation should\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# take less than 1 ns. (And this prevents divide by zero errors.)\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer\u001b[38;5;241m.\u001b[39mtimeit(number), \u001b[38;5;241m1e-9\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Pytorch/lib/python3.11/timeit.py:178\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    176\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner(it, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 1.96 GiB total capacity; 328.13 MiB already allocated; 1.58 GiB free; 342.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lets explore the speed of each of the 3 implementations\n",
    "from torch.backends.cuda import sdp_kernel, SDPBackend\n",
    "\n",
    "# Helpful arguments mapper\n",
    "backend_map = {\n",
    "    SDPBackend.MATH: {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n",
    "    SDPBackend.FLASH_ATTENTION: {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n",
    "    SDPBackend.EFFICIENT_ATTENTION: {\n",
    "        \"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n",
    "}\n",
    "\n",
    "with sdp_kernel(**backend_map[SDPBackend.MATH]):\n",
    "    print(f\"The math implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "\n",
    "\n",
    "with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n",
    "    try:\n",
    "        print(f\"The flash attention implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"FlashAttention is not supported. See warnings for reasons.\")\n",
    "\n",
    "with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n",
    "    try:\n",
    "        print(f\"The memory efficient implementation runs in {benchmark_torch_function_in_microseconds(F.scaled_dot_product_attention, query, key, value):.3f} microseconds\")\n",
    "    except RuntimeError:\n",
    "        print(\"EfficientAttention is not supported. See warnings for reasons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54527e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads : int, embed_dimension:int, bias:bool=False, is_causal:bool=False, dropout:float=0.0 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert embed_dimension % num_heads == 0\n",
    "        self.c_attn = nn.Linear(embed_dimension, 3 * embed_dimension, bias = bias)\n",
    "        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dimension = embed_dimension\n",
    "        \n",
    "        self.is_causal = is_causal\n",
    "        \n",
    "    def forward():\n",
    "        \n",
    "        query_projected = self.attn(x)\n",
    "        \n",
    "        batch_size = query_projected.size(0)\n",
    "        embed_dim = query_projected.size(2)\n",
    "        head_dim = embed_dim // (self.num_heads *3)\n",
    "        \n",
    "        \n",
    "        query, key, value = query_projected.chunk(3,1)\n",
    "        query = query.view(batch_size, -1, sel.num_heads, head_dim).transpose(1,2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1,2)\n",
    "        \n",
    "        if self.training:\n",
    "            dropout = self.dropout\n",
    "            is_causal = self.is_causal\n",
    "        else:\n",
    "            dropout = 0.0\n",
    "            is_causal = False\n",
    "        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p= dropout, is_causal=is_causal)\n",
    "        y = y.transpose(1,2).view(batch_size, -1, self.num_heads*head_dim)\n",
    "        \n",
    "        y =self.resid_dropou(self.c_proj(y))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f32af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
      "  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_heads = 8\n",
    "heads_per_dim = 64\n",
    "embed_dimension = num_heads * heads_per_dim\n",
    "dtype = torch.float16\n",
    "model = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3cbb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
